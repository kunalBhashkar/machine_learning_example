{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this, because given our current deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_and_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_and_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat\n",
    "  for t in range(100):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method\n",
    "# NOTE: this script implements the Monte Carlo Exploring-Starts method\n",
    "#       for finding the optimal policy\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this if we have a deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  # this is called the \"exploring starts\" method\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  a = np.random.choice(ALL_POSSIBLE_ACTIONS) # first action is uniformly random\n",
    "\n",
    "  # be aware of the timing\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  seen_states = set()\n",
    "  seen_states.add(grid.current_state())\n",
    "  num_steps = 0\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    num_steps += 1\n",
    "    s = grid.current_state()\n",
    "\n",
    "    if s in seen_states:\n",
    "      # hack so that we don't end up in an infinitely long episode\n",
    "      # bumping into the wall repeatedly\n",
    "      # if num_steps == 1 -> bumped into a wall and haven't moved anywhere\n",
    "      #   reward = -10\n",
    "      # else:\n",
    "      #   reward = falls off by 1 / num_steps\n",
    "      reward = -10. / num_steps\n",
    "      states_actions_rewards.append((s, None, reward))\n",
    "      break\n",
    "    elif grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = policy[s]\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "    seen_states.add(s)\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method\n",
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  # put this into a function since we are using it so often\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90| 1.00|\n",
      "---------------------------\n",
      "-0.90| 0.00|-0.90|-1.00|\n",
      "---------------------------\n",
      "-0.90|-0.90|-0.90|-0.90|\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAZTklEQVR4nO3deXic5Xnv8e9tSV7kTZYt40U2smODCYvBKMFh8SFmCTEUh6TtIW0S0pbjk3MaGk7TK4GQc5KQkkLS5pykSZuapQkJCaRAlgIONmAWEzDIYLwT78a2bMubbEu21vv8Ma/skWZG0iwa+ZF+n+vSpdGjd7nnndFPzzzv886YuyMiIuEZ0NsFiIhIZhTgIiKBUoCLiARKAS4iEigFuIhIoArzubMxY8Z4RUVFPncpIhK8FStW7Hf3so7teQ3wiooKqqqq8rlLEZHgmdn2ZO0aQhERCZQCXEQkUApwEZFAKcBFRAKlABcRCVSXAW5mD5nZPjNbE9dWamZLzGxj9H1Uz5YpIiIddacH/mPgug5tdwDPu/t04PnoZxERyaMu54G7+8tmVtGheT5wZXT7J8CLwJdzWFc7tcebuOfpdbjDsMGFzJlexodnjO2p3YmIBCHTC3nOcPdqAHevNrOUaWpmC4AFAJMnT85oZ1/85Ts8t37vyZ///dVtbLv3+oy2JSLSV/T4SUx3X+jule5eWVaWcCVot+w+fDzHVYmIhC/TAN9rZuMBou/7cldSIn1mkIhIokwD/LfALdHtW4Df5KYcERHpru5MI/wF8BpwtpntNLO/Au4FrjGzjcA10c8iIpJH3ZmF8skUv7oqx7WIiEgadCWmiEiggghwd53GFBHpKIgAFxGRRApwEZFAKcBFRAKlABcRCZQCXEQkUApwEZFAKcBFRAKlABcRCVQQAa7reEREEgUR4CIikkgBLiISKAW4iEigFOAiIoEKIsBdH6omIpIgiAAXEZFECnARkUApwEVEAhVEgOtCHhGRREEEuIiIJFKAi4gESgEuIhIoBbiISKCCCHCdwxQRSRREgIuISCIFuIhIoBTgIiKBUoCLiAQqiAB3XYopIpIgiAAXEZFEWQW4mf0vM1trZmvM7BdmNjhXhYmISOcyDnAzmwj8DVDp7ucBBcDNuSpMREQ6l+0QSiEwxMwKgWJgd/YlJdIIuIhIoowD3N13Af8I7ACqgVp3X9xxOTNbYGZVZlZVU1OTeaUiItJONkMoo4D5wBRgAjDUzD7VcTl3X+jule5eWVZWlnmlIiLSTjZDKFcDW929xt2bgCeBS3NTloiIdCWbAN8BzDazYjMz4CpgfW7KEhGRrmQzBr4ceBx4C1gdbWthjurqsLMe2aqISNAKs1nZ3b8GfC1HtYiISBp0JaaISKAU4CIigVKAi4gEKogA1zlMEZFEQQS4iIgkUoCLiARKAS4iEqggAlyfyCMikiiIABcRkURBBHjsrVZERCReEAGuIRQRkURBBLiIiCQKIsDV/xYRSRREgIuISCIFuIhIoBTgIiKBUoCLiAQqiADXLEIRkURBBLiIiCRSgIuIBEoBLiISqCAC3HUpj4hIgiACXEREEgUR4IbejVBEpKMgAlxDKCIiiYIIcBERSRREgOtCHhGRREEEuIiIJFKAi4gESgEuIhKorALczErM7HEz22Bm683sQ7kqTEREOleY5frfA37n7n9sZgOB4hzUJCIi3ZBxgJvZCGAO8FkAd28EGnNTloiIdCWbIZSpQA3w72b2tpk9YGZDOy5kZgvMrMrMqmpqajLakaYRiogkyibAC4FZwL+6+0VAHXBHx4XcfaG7V7p7ZVlZWRa7ExGReNkE+E5gp7svj35+nFigi4hIHmQc4O6+B3jPzM6Omq4C1uWkKhER6VK2s1BuAx6JZqBsAf4i+5JERKQ7sgpwd18JVOaoFhERSYOuxBQRCZQCXEQkUApwEZFABRHgrit5REQSBBHgIiKSKIgA3117IqHtu4vf5cCxBg7WNfLT17blvSYRkd6W7TzwXvP9FzaBGW/vOMQrG/fzgSmlzBg3orfLEhHJmyB64P/zyvclbT96oolD9bE3QGxq1ji5iPQvQQS4iIgkUoCLiAQqiAA3S94eP7vQ0RCKiPQvQQS4iIgkCiLAjeRd8PieeaplRET6qiACPBVdoCki/VkQAZ5qDFxEpD8LI8B7uwARkdNQEAEuIiKJwgjwboyhaBqhiPQ3YQS4iIgk6DMBrmmEItLfBBHgimYRkURBBLiIiCTqMwGuk5gi0t8EEeC6kEdEJFEYAa5RcBGRBEEEuIiIJAoiwLszhKJeuoj0N0EEeHfoJKaI9Dd9JsBFRPqbIAJcgyMiIomCCHAREUmUdYCbWYGZvW1mT+WiIBER6Z5c9MC/AKzPwXZS0oU8IiKJsgpwMysHrgceyE05IiLSXdn2wP8f8CWgNdUCZrbAzKrMrKqmpiajnVgnXXDN/xaR/irjADezG4B97r6is+XcfaG7V7p7ZVlZWaa7S719zf8WkX4qmx74ZcCNZrYNeBSYa2Y/y0lVIiLSpYwD3N3vdPdyd68AbgZecPdP5awyERHpVBDzwFMNgbtr+ERE+q/CXGzE3V8EXszFtpLp7ESlTmKKSH8VRA9cREQSKcBFRAIVRICnGgM3M00jFJF+K4wAT9Guk5gi0p8FEeCKaRGRREEEuIiIJAoiwFONlPzkte20pHwXFhGRvi2IAO/M3iMnersEEZFeEXyA60SmiPRX4Qd49H3vkYZerUNEJN+CCPDO5nq3dcD/28NVeapGROT0EESAd0ZDKCLSX4Uf4L1dgIhILwk+wEVE+qvwA1xdcBHpp4II8M6GuZXfItJfBRHgndFJTBHpr8IP8N4uQESkl4Qf4HEJfrCusfcKERHJs/ADPK4PPuubSzhwTFdkikj/EHyAd3SoXr1wEekfgg9wncMUkf4q+ABvaE7/DcHrG5vZr6EWEQlc8AGeiRu+v4zKv3+ut8sQEclKEAGe67neW/bX5XR7IiK9IYgAT0+qz7AXEelb+mCAi4j0D/06wFtbNYVFRMIVRID31FTBHyzd1DMbFhHJgyACPB2WxhD4qp21PVeIiEgPyzjAzWySmS01s/VmttbMvpDLwjKlC3tEpL8ozGLdZuCL7v6WmQ0HVpjZEndfl6PaRESkExn3wN292t3fim4fBdYDE3NVmIiIdC4nY+BmVgFcBCxP8rsFZlZlZlU1NTUZbT+dUZF0xsBFREKWdYCb2TDgCeB2dz/S8ffuvtDdK929sqysLNvd5ZTCXkRCllWAm1kRsfB+xN2fzE1JIiLSHdnMQjHgQWC9u383dyWJiEh3ZNMDvwz4NDDXzFZGX/NyVFc7mhooIpIo42mE7r4MvXOUiEiv6XNXYoqI9Bd9LsBTvSRYvHYP9Y3Nea1FRKQnBRHgntZM8ETv7jnKgp+u4CtPrs5RRSIivS+IAM/WsYYmALYfrG/XrgF8EQlZvwhwRbWI9EV9LsCTD7ZoHqKI9D19L8CV1SLSTwQR4OmFshJcRPqHIAI8VzQSLiJ9SZ8L8HR66+qri0jI+l6A93YBIiJ5EkSAZxvKOrEpIn1REAGejnTCWmPiIhKyvhfgGkQRkX4iiAD/k4vLGVjQvVI764GbPkNNRPqQIAJ8UmkxK/731b1dhojIaSXjD3Q4XcX3wOsamjn3a89y9TljO11nx4F6SocNZNigxMNxuL6R+sYWJpQMyXWpIiJZCaIHDt2fiRI/Br79QOzdB59bv6/TdeZ8Zyl/dv/rSX93+X1LufTeF7q5dxGR/AkmwJOZf+GEhLa2HvjPl+9g3vdf6XT9xev24tEKq3bWJl3mWIM+BEJETk/BBHiy049DigpSLv/tZzd0a7upgltE5HQXTIAnG0LJxaSSJev2Zr8REZFeEEyAd1e6V13+YOmmnilERKSHBRPgyYM5sQvuOIfrGzlc35R0O1tqjuW2sE7M+94rXKYToCLSQ/rcNMLa4018d8kfkv5uxfZDzP2nl/JWy7rqIxmv+857h9l3tIFr3n9GDisSkb4kmADv7nj3px98I+N9HK5vZOigQoq6uOrzUF0jR080M3l0ccb76sr8H74KwLZ7r++xfYhI2IIeQsn1lfEX3r2E6XctSvq7+sZmKu54mgde2cKV//gic76zNK1t1xxt4Nr/+xI7ornpIiLZCibA88mT/LeoPR4bU7//lS0nbwO0tjoHjjV0ur29R07wgXue4w97j/HQq1uzqu1nr2/nRy9tzmobItI3KMCTaG51fln1HrO/9fzJNotOmLa0tl/2vmc3cPHfP8fh+saU2/th3EyX5tbWhN/f/Z/r+Msfv5l03W//rv189q/+eg33LureHHcR6dsU4Emc9dVFfOnxVew5cuJk2+tbDgDQEhfAa3bV8m8vbQHg6InUV2w+/Nr2k7ebmhN79w+9upUXNpy63P+Xb7538va/vLiZ5pZWlm3cn3Tbuw8fp+KOp/ndmj1d3S0R6WOCDvCeenPYZOPttz+2EoC6xpaTbTf887KE5d7YepDdh4+n3PZjVe/R2pp6svqJpha+9MSqdm3T7lrEpx5cztK4kH9q1W4AVr53GIDP/WwF2w/UJWyvrqG53ZCPiPQdYQd4L7y9d2Nz4hAIQENzK4frG/nTf3uNS+99gV+/vSvlNu55Zj31jc2s2nm4XZg/9uYO3th6MOV68a8IPv/zt4FY4Ld5NK7n3mbOt5cy8xuL+eIv32Hhy5u5/dG3OXqiiRff3ceb2w6yYvup/bl7Xt775XB9I/e/vIWDdamHnUSka1lNIzSz64DvAQXAA+5+b06qSuY0/6Cdmxe+xv5jpwKprceezIPLtvLgssSTmV9+YnWn+/jGf65t9/P66iMcjwvwAUn+oR2IQvKJt3aebPv95gPsO3rqxOsbX7mK+1/Zwv2vxGqaMW4493+mkkml7adJPrO6mnsXbeAb88/lw2e3f4veXYePMzHuLXe3H6hjctz68R+mceeTq1m0Zg/3PLOee246j7t+tYalf3clP1y6icmlxdxwwXgKBhgDzJhQMoSCZHesl+07coKiggGMGjow4200t7TS3OoM7uQ9fUQ6Y8lmXHRrRbMC4A/ANcBO4E3gk+6+LtU6lZWVXlVVldH+auubmHn34nZtn5o9mZ+9viOj7fUVl08bw7JNycfHu+vCSSUnh2JSuf6C8Ty9qrpd21fmzWDa2GEs23ig3eya95UNZXNNHSMGF3IkOjdw3bnjqNp+sN0/uXR8YlY51bXHKRs+iN+s3M3154/nyrPLqDnWwOZ9dew4WMcfX1zO7zcf4GBdIx8+eyx3P7WOmeUj+R9XTmNzzTHW7q7lYxdOpHxUMXc/tZbBRQXcNncai9ft5ba501m9s5ZHlm+n8sxRTB5dzBtbDzFrcglv7TjM3BljmVAymG3761m9q5b7opPLW/9hHnc8sZoDdY1866bzGFg4gIbmVjbtO8Zl08awbveRk68U//tPV/C9my+kYIDx8GvbeXxF7J/qlm/N49XN+xlVPJDq2hNMLRvKo2/s4L9+YBKPvvEeDyzbylevP4cbZ05g7IjBPL2qmvt+t4HLpo3mwLFGRg4p4psfO4/n1u+leGABF59ZysG6Rk40tbBozR5GFRdR39jCpn3HuHPeDL719HpumlXO1DFDGT1sIG/vOMwlU0ppcaehuZUHXt7CJVNHc0H5SB5ZvoPS4oHsPFTPql213HPT+YwdPoiX3q1h3MjBnDN+BCeaWmhobuXp1dWUlwxhZHERY4cPouZoAzPGjaCowCgYYBw50UxzSytNLc7Y4YP4/eYDNLW2cqiukUumjuaM4YNocWfNriOMHT6IsuGDGFxUQEur8/z6vVxQXsIAg6dWVfPxWRM5eqKZ8lFDMDOON7bwwoZ9TBkzlKllQ3GHogKjMLqmo7a+iZ2H6zln3Aj21zWwdtcR/stZZfx65S5mTiph0qhiWqP7P7hoAGt2HeHCSSUcONZAde0JKsYM5XB9I+NHDqGxpZXBhQMoLBjAntoTbN0fG7ocVDSAmeUlNLe2sr76KFPLhtLU3ErxwEJ2HKzn7HHDM3ruA5jZCnevTGjPIsA/BHzd3T8S/XwngLv/Q6p1sgrw403M/Eb7AL/18ik8kKQnK9JXDR1Y0O48TF9XMbqYbV1cO1EW/bNIZcywgRl3HHLp1399GRdOKslo3VQBns0Y+EQgftB1Z9TWcccLzKzKzKpqamoy3tnIIUV8+boZ/M3caQBcMqWUz135Pv7sksn8/NZLMCPpwblx5gQ+et44Pj37TP72mrN44DOVXH/BeK4+59Ql6n80s/37il8ypZRHbr2Ez15awfBBhXxz/rkAfPbSCgYWDmDmpBLGjRjcrbqHDoy9PB4+qJAFc6Zy6+VTTv7urDOGMbVs6Ml9njFiEAAzo/tRMbqY8yeO5J6bzku67UmlsSGL2+ZO41/+fBYQG0aZEfeffnJp8ckaINZjb1NU0H5o4oMVpcydMTbpfRue5NOKOvrErPIul+loYGH7p+Btc6e1G4ppU1RgnDdxBONHdu+4p3LR5BI+flHC07Rbkh2X2VNLgVNvbXzF9DGMHFIExD7Lte12Z3XHPz4AwwefOtZXTD/1eJnBDRdM4NwJI7pVW5u251W86WOHtVtv5qQShhQVtBv2Omd84n4umzaaARZ7PsTX1nY/OyocYFxQPpJro7eEOHN0MSOHFCU8V2ZOKmHOWWXtnrujhw7k/PISPha97//wwYWcN3FEu/t0xfQxfGjq6JP3p6PZU0u5LO45X9GNq6fnnFUGtH9c2v4mJ5YMOTlU2fbYdzR2eKy2cyeM4ILykUwsGcL1549nZvnILvedrmx64H8CfMTdb41+/jTwQXe/LdU62fTARUT6q57oge8EJsX9XA7szmJ7IiKShmwC/E1guplNMbOBwM3Ab3NTloiIdCXjaYTu3mxmnweeJTaN8CF3X9vFaiIikiNZzQN392eAZ3JUi4iIpCHoKzFFRPozBbiISKAU4CIigVKAi4gEKuMLeTLamVkNsL3LBZMbA2T3ph89Q3WlR3WlR3Wl53StC7Kr7Ux3L+vYmNcAz4aZVSW7Eqm3qa70qK70qK70nK51Qc/UpiEUEZFAKcBFRAIVUoAv7O0CUlBd6VFd6VFd6Tld64IeqC2YMXAREWkvpB64iIjEUYCLiAQqiAA3s+vM7F0z22Rmd+Rxv5PMbKmZrTeztWb2haj962a2y8xWRl/z4ta5M6rzXTP7SA/Xt83MVkc1VEVtpWa2xMw2Rt9HRe1mZt+PaltlZrN6qKaz447LSjM7Yma398YxM7OHzGyfma2Ja0v7+JjZLdHyG83slh6q6ztmtiHa96/MrCRqrzCz43HH7Udx61wcPf6botqz+vTnFHWl/bjl+u81RV2PxdW0zcxWRu35PF6p8iF/zzF3P62/iL1V7WZgKjAQeAd4f572PR6YFd0eTuxDnN8PfB34uyTLvz+qbxAwJaq7oAfr2waM6dD2beCO6PYdwH3R7XnAIsCA2cDyPD12e4Aze+OYAXOAWcCaTI8PUApsib6Pim6P6oG6rgUKo9v3xdVVEb9ch+28AXwoqnkR8NEeqCutx60n/l6T1dXh9/8E/J9eOF6p8iFvz7EQeuAfBDa5+xZ3bwQeBebnY8fuXu3ub0W3jwLrSfK5n3HmA4+6e4O7bwU2Eas/n+YDP4lu/wT4WFz7wx7zOlBiZuN7uJargM3u3tnVtz12zNz9ZeBgkv2lc3w+Aixx94PufghYAlyX67rcfbG7N0c/vk7sE65Simob4e6veSwFHo67LzmrqxOpHrec/712VlfUi/5T4BedbaOHjleqfMjbcyyEAO/Whyf3NDOrAC4ClkdNn49eBj3U9hKJ/NfqwGIzW2FmC6K2M9y9GmJPMGBsL9UGsU9piv/DOh2OWbrHpzeO218S66m1mWJmb5vZS2Z2RdQ2MaolH3Wl87jl+3hdAex1941xbXk/Xh3yIW/PsRACPNk4VV7nPprZMOAJ4HZ3PwL8K/A+4EKgmthLOMh/rZe5+yzgo8Bfm9mcTpbNa20W+5i9G4H/iJpOl2OWSqo68n3c7gKagUeipmpgsrtfBPwt8HMzG5HHutJ93PL9eH6S9p2EvB+vJPmQctEUNWRcWwgB3qsfnmxmRcQenEfc/UkAd9/r7i3u3grcz6mX/Hmt1d13R9/3Ab+K6tjbNjQSfd/XG7UR+6fylrvvjWo8LY4Z6R+fvNUXnby6Afjz6GU+0RDFgej2CmLjy2dFdcUPs/RIXRk8bvk8XoXAx4HH4urN6/FKlg/k8TkWQoD32ocnR+NrDwLr3f27ce3xY8c3AW1nx38L3Gxmg8xsCjCd2ImTnqhtqJkNb7tN7CTYmqiGtrPYtwC/iavtM9GZ8NlAbdvLvB7Srmd0OhyzuP2lc3yeBa41s1HR8MG1UVtOmdl1wJeBG929Pq69zMwKottTiR2fLVFtR81sdvQ8/UzcfcllXek+bvn8e70a2ODuJ4dG8nm8UuUD+XyOZXMWNl9fxM7e/oHYf9O78rjfy4m9lFkFrIy+5gE/BVZH7b8Fxsetc1dU57tkeZa7i9qmEjvD/w6wtu24AKOB54GN0ffSqN2AH0a1rQYqe7C2YuAAMDKuLe/HjNg/kGqgiVgv568yOT7ExqQ3RV9/0UN1bSI2Dtr2PPtRtOwnosf3HeAt4I/itlNJLFA3Az8gurI6x3Wl/bjl+u81WV1R+4+Bz3VYNp/HK1U+5O05pkvpRUQCFcIQioiIJKEAFxEJlAJcRCRQCnARkUApwEVEAqUAFxEJlAJcRCRQ/x9KehSbk8zE/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "final values:\n",
      "---------------------------\n",
      "-1.77|-0.83| 1.00| 0.00|\n",
      "---------------------------\n",
      "-2.67| 0.00|-0.66| 0.00|\n",
      "---------------------------\n",
      "-3.15|-2.77|-1.69|-1.00|\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.9)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0 # needs to be initialized to something so we can argmax it\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(2000):\n",
    "    if t % 100 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # update policy\n",
    "    for s in policy.keys():\n",
    "      policy[s] = max_dict(Q[s])[0]\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)\n",
    "\n",
    "  # find V\n",
    "  V = {}\n",
    "  for s, Qs in Q.items():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo no es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "from monte_carlo_es import max_dict\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: find optimal policy and value function\n",
    "#       using on-policy first-visit MC\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # choose given a with probability 1 - eps + eps/4\n",
    "  # choose some other a' != a with probability eps/4\n",
    "  p = np.random.random()\n",
    "  # if p < (1 - eps + eps/len(ALL_POSSIBLE_ACTIONS)):\n",
    "  #   return a\n",
    "  # else:\n",
    "  #   tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "  #   tmp.remove(a)\n",
    "  #   return np.random.choice(tmp)\n",
    "  #\n",
    "  # this is equivalent to the above\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "  # in this version we will NOT use \"exploring starts\" method\n",
    "  # instead we will explore using an epsilon-soft policy\n",
    "  s = (2, 0)\n",
    "  grid.set_state(s)\n",
    "  a = random_action(policy[s])\n",
    "\n",
    "  # be aware of the timing\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    if grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = random_action(policy[s]) # the next state is stochastic\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final values:\n",
      "---------------------------\n",
      " 0.58| 0.78| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.41| 0.00| 0.76| 0.00|\n",
      "---------------------------\n",
      " 0.25| 0.10|-0.23| 0.00|\n",
      "final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  D  |\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-0.1)\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions: # not a terminal state\n",
    "      Q[s] = {}\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(5000):\n",
    "    if t % 1000 == 0:\n",
    "      print(t)\n",
    "\n",
    "    # generate an episode using pi\n",
    "    biggest_change = 0\n",
    "    states_actions_returns = play_game(grid, policy)\n",
    "\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      sa = (s, a)\n",
    "      if sa not in seen_state_action_pairs:\n",
    "        old_q = Q[s][a]\n",
    "        returns[sa].append(G)\n",
    "        Q[s][a] = np.mean(returns[sa])\n",
    "        biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "        seen_state_action_pairs.add(sa)\n",
    "    deltas.append(biggest_change)\n",
    "\n",
    "    # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "    for s in policy.keys():\n",
    "      a, _ = max_dict(Q[s])\n",
    "      policy[s] = a\n",
    "\n",
    "  plt.plot(deltas)\n",
    "  plt.show()\n",
    "\n",
    "  # find the optimal state-value function\n",
    "  # V(s) = max[a]{ Q(s,a) }\n",
    "  V = {}\n",
    "  for s in policy.keys():\n",
    "    V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  print(\"final values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"final policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from grid_world import standard_grid, negative_grid\n",
    "from iterative_policy_evaluation import print_values, print_policy\n",
    "\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def random_action(a):\n",
    "  # choose given a with probability 0.5\n",
    "  # choose some other a' != a with probability 0.5/3\n",
    "  p = np.random.random()\n",
    "  if p < 0.5:\n",
    "    return a\n",
    "  else:\n",
    "    tmp = list(ALL_POSSIBLE_ACTIONS)\n",
    "    tmp.remove(a)\n",
    "    return np.random.choice(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "\n",
    "  # reset game to start at a random position\n",
    "  # we need to do this, because given our current deterministic policy\n",
    "  # we would never end up at certain states, but we still want to measure their value\n",
    "  start_states = list(grid.actions.keys())\n",
    "  start_idx = np.random.choice(len(start_states))\n",
    "  grid.set_state(start_states[start_idx])\n",
    "\n",
    "  s = grid.current_state()\n",
    "  states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "  while not grid.game_over():\n",
    "    a = policy[s]\n",
    "    a = random_action(a)\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    states_and_rewards.append((s, r))\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_and_returns = []\n",
    "  first = True\n",
    "  for s, r in reversed(states_and_rewards):\n",
    "    # the value of the terminal state is 0 by definition\n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_and_returns.append((s, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_and_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_and_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "values:\n",
      "---------------------------\n",
      " 0.43| 0.56| 0.73| 0.00|\n",
      "---------------------------\n",
      " 0.33| 0.00| 0.23| 0.00|\n",
      "---------------------------\n",
      " 0.26| 0.20| 0.12|-0.16|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "if __name__ == '__main__':\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  grid = standard_grid()\n",
    "\n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  # state -> action\n",
    "  # found by policy_iteration_random on standard_grid\n",
    "  # MC method won't get exactly this, but should be close\n",
    "  # values:\n",
    "  # ---------------------------\n",
    "  #  0.43|  0.56|  0.72|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.33|  0.00|  0.21|  0.00|\n",
    "  # ---------------------------\n",
    "  #  0.25|  0.18|  0.11| -0.17|\n",
    "  # policy:\n",
    "  # ---------------------------\n",
    "  #   R  |   R  |   R  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |      |   U  |      |\n",
    "  # ---------------------------\n",
    "  #   U  |   L  |   U  |   L  |\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'L',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "  }\n",
    "\n",
    "  # initialize V(s) and returns\n",
    "  V = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "    if s in grid.actions:\n",
    "      returns[s] = []\n",
    "    else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      V[s] = 0\n",
    "\n",
    "  # repeat until convergence\n",
    "  for t in range(5000):\n",
    "\n",
    "    # generate an episode using pi\n",
    "    states_and_returns = play_game(grid, policy)\n",
    "    seen_states = set()\n",
    "    for s, G in states_and_returns:\n",
    "      # check if we have already seen s\n",
    "      # called \"first-visit\" MC policy evaluation\n",
    "      if s not in seen_states:\n",
    "        returns[s].append(G)\n",
    "        V[s] = np.mean(returns[s])\n",
    "        seen_states.add(s)\n",
    "\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
